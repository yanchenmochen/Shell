---------------------------------------------------------------------
评测框架：opencompass（0.4.0版本基础上修改）

---------------------------------------------------------------------
1、base模型mmlu ppl数据集配置：mmlu_ppl_ac766d

```
from opencompass.openicl.icl_prompt_template import PromptTemplate
from opencompass.openicl.icl_retriever import FixKRetriever
from opencompass.openicl.icl_inferencer import PPLInferencer
from opencompass.openicl.icl_evaluator import AccwithDetailsEvaluator
from opencompass.datasets import MMLUDataset

# None of the mmlu dataset in huggingface is correctly parsed, so we use our own dataset reader
# Please download the dataset from https://people.eecs.berkeley.edu/~hendrycks/data.tar

mmlu_reader_cfg = dict(
    input_columns=['input', 'A', 'B', 'C', 'D'],
    output_column='target',
    train_split='dev')

mmlu_all_sets = [
    'college_biology',
    'college_chemistry',
    'college_computer_science',
    'college_mathematics',
    'college_physics',
    'electrical_engineering',
    'astronomy',
    'anatomy',
    'abstract_algebra',
    'machine_learning',
    'clinical_knowledge',
    'global_facts',
    'management',
    'nutrition',
    'marketing',
    'professional_accounting',
    'high_school_geography',
    'international_law',
    'moral_scenarios',
    'computer_security',
    'high_school_microeconomics',
    'professional_law',
    'medical_genetics',
    'professional_psychology',
    'jurisprudence',
    'world_religions',
    'philosophy',
    'virology',
    'high_school_chemistry',
    'public_relations',
    'high_school_macroeconomics',
    'human_sexuality',
    'elementary_mathematics',
    'high_school_physics',
    'high_school_computer_science',
    'high_school_european_history',
    'business_ethics',
    'moral_disputes',
    'high_school_statistics',
    'miscellaneous',
    'formal_logic',
    'high_school_government_and_politics',
    'prehistory',
    'security_studies',
    'high_school_biology',
    'logical_fallacies',
    'high_school_world_history',
    'professional_medicine',
    'high_school_mathematics',
    'college_medicine',
    'high_school_us_history',
    'sociology',
    'econometrics',
    'high_school_psychology',
    'human_aging',
    'us_foreign_policy',
    'conceptual_physics',
]

mmlu_datasets = []
for _name in mmlu_all_sets:
    _hint = f'The following are multiple choice questions (with answers) about {_name.replace("_", " ")}.\n\n'
    question_overall = '{input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n'
    mmlu_infer_cfg = dict(
        ice_template=dict(
            type=PromptTemplate,
            template={opt: f'{question_overall}Answer: {opt}\n\n' for opt in ['A', 'B', 'C', 'D']},
        ),
        prompt_template=dict(
            type=PromptTemplate, 
            template={opt: f'{_hint}</E>{question_overall}Answer: {opt}' for opt in ['A', 'B', 'C', 'D']},
            ice_token='</E>',
        ),
        retriever=dict(type=FixKRetriever, fix_id_list=[0, 1, 2, 3, 4]),
        inferencer=dict(type=PPLInferencer),
    )

    mmlu_eval_cfg = dict(evaluator=dict(type=AccwithDetailsEvaluator), )

    mmlu_datasets.append(
        dict(
            abbr=f'lukaemon_mmlu_{_name}_ppl',
            type=MMLUDataset,
            path='data/mmlu',
            name=_name,
            reader_cfg=mmlu_reader_cfg,
            infer_cfg=mmlu_infer_cfg,
            eval_cfg=mmlu_eval_cfg,
        ))

del _name, _hint
```
---------------------------------------------------------------------
ppl评测需要BS=1, num_worker=1

        models = [
            dict(
                type=opencompass.models.VLLM,
                abbr="MODEL_NAME",
                path="MODEL_PATH", #模型路径
                model_kwargs=dict(gpu_memory_utilization=0.7, tensor_parallel_size=1),
                generation_kwargs=dict(temperature=0, top_p=1, top_k=-1, repetition_penalty=1),
                max_out_len=1,
                max_seq_len=8192,
                batch_size=1,
                run_cfg=dict(num_gpus=1, num_procs=1),
            ),
        ]

    infer = dict(
        partitioner=dict(type=NumWorkerPartitioner, num_worker=1),
        runner=dict(
            type=LocalRunner, max_num_workers=1024,
            task=dict(type=OpenICLInferTask)),
    )

    eval = dict(
        partitioner=dict(type=NaivePartitioner),
        runner=dict(type=LocalRunner,
                max_num_workers=64,
                task=dict(type=OpenICLEvalTask)),
    )

---------------------------------------------------------------------
2、chat模型mmlu数据集配置:

```
from opencompass.openicl.icl_prompt_template import PromptTemplate
from opencompass.openicl.icl_retriever import FixKRetriever, ZeroRetriever
from opencompass.openicl.icl_inferencer import GenInferencer
from opencompass.openicl.icl_evaluator import AccwithDetailsEvaluator
from opencompass.datasets import MMLUDataset
from opencompass.utils.text_postprocessors import first_option_postprocess, first_capital_after_answer

# None of the mmlu dataset in huggingface is correctly parsed, so we use our own dataset reader
# Please download the dataset from https://people.eecs.berkeley.edu/~hendrycks/data.tar

mmlu_reader_cfg = dict(
    input_columns=['input', 'A', 'B', 'C', 'D'],
    output_column='target',
    train_split='dev')

mmlu_all_sets = [
    'college_biology',
    'college_chemistry',
    'college_computer_science',
    'college_mathematics',
    'college_physics',
    'electrical_engineering',
    'astronomy',
    'anatomy',
    'abstract_algebra',
    'machine_learning',
    'clinical_knowledge',
    'global_facts',
    'management',
    'nutrition',
    'marketing',
    'professional_accounting',
    'high_school_geography',
    'international_law',
    'moral_scenarios',
    'computer_security',
    'high_school_microeconomics',
    'professional_law',
    'medical_genetics',
    'professional_psychology',
    'jurisprudence',
    'world_religions',
    'philosophy',
    'virology',
    'high_school_chemistry',
    'public_relations',
    'high_school_macroeconomics',
    'human_sexuality',
    'elementary_mathematics',
    'high_school_physics',
    'high_school_computer_science',
    'high_school_european_history',
    'business_ethics',
    'moral_disputes',
    'high_school_statistics',
    'miscellaneous',
    'formal_logic',
    'high_school_government_and_politics',
    'prehistory',
    'security_studies',
    'high_school_biology',
    'logical_fallacies',
    'high_school_world_history',
    'professional_medicine',
    'high_school_mathematics',
    'college_medicine',
    'high_school_us_history',
    'sociology',
    'econometrics',
    'high_school_psychology',
    'human_aging',
    'us_foreign_policy',
    'conceptual_physics',
]

mmlu_datasets = []
for _name in mmlu_all_sets:
    _hint = """
Answer the following multiple choice question. The last line of your response should be of the following format: 'ANSWER: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.

{input}

A) {A}
B) {B}
C) {C}
D) {D}
""".strip()
    mmlu_infer_cfg = dict(
        prompt_template=dict(
            type=PromptTemplate,
            template=dict(
                round=[
                    dict(role='HUMAN', prompt=f'{_hint}'),
                ],
            ),
        ),
        retriever=dict(type=ZeroRetriever),
        inferencer=dict(type=GenInferencer),
    )

    mmlu_eval_cfg = dict(
        evaluator=dict(type=AccwithDetailsEvaluator),
        pred_postprocessor=dict(type=first_capital_after_answer, options='ABCD', think_token='</think>', inBox=False))

    mmlu_datasets.append(
        dict(
            abbr=f'lukaemon_mmlu_{_name}_dsv3',
            type=MMLUDataset,
            path='data/mmlu',
            name=_name,
            reader_cfg=mmlu_reader_cfg,
            infer_cfg=mmlu_infer_cfg,
            eval_cfg=mmlu_eval_cfg,
        ))
```

---------------------------------------------------------------------
后处理函数：(为了能同时处理reasoning模型和chat模型答案抽取做了修改)
def first_capital_after_answer(text: str, options: str, think_token: str = '</think>', simple: bool = False, inBox: bool = True) -> str:
    if text=='':
        return ''

    import re
    text = re.sub(r'assistant', 'assistant', text, flags=re.IGNORECASE)
    ans_flag=0
    raw_text = text
    try:
        if simple:
            for tt in text:
                if tt in options:
                    return tt
                break
    except:
        pass

    if inBox:
        import re
        pattern = r'\{(.*?)\}'
        result = re.findall(pattern, text)
        try:
            for tt in result[-1]:
                if tt in options:
                    return tt
        except:
            pass

    text = re.sub(r'</answer>', '',  text, flags=re.IGNORECASE)
    text = re.sub(r'<answer>', '', text, flags=re.IGNORECASE)
    text = re.sub(r'answer', 'ANSWER', text, flags=re.IGNORECASE)
    parts = text.split('ANSWER')
    ans = []
    for p in parts:
        ans += p.split('答案')

    if len(ans) != 1:
        ans_line = ans[-1].strip()
        upper = False
        for char in ans_line:
            if char in options:
                upper = True
                ans_flag=1
                return char
        if upper==False:
            pattern = r'^[a-zA-Z]$'
            ans_line = ans[-2].strip()
            for i in range(len(ans_line)):
                if ans_line[i] in options:
                    pattern = r'^[a-zA-Z]$' 
                    try:
                        if (re.match(pattern, ans_line[i+1]) is None) and i+1<len(ans_line):
                            ans_flag=1
                            return ans_line[i]
                    except:
                        pass
    else:
        for char in ans:
            if char in options:
                ans_flag=1
                return char
    if ans_flag==0:
        for tt in raw_text:
            if tt in options:
                return tt
    return ''

---------------------------------------------------------------------
评测采用api调用方式，用sglang（v0.4.6post5-cu124）部署模型进行评测

python3 -m sglang.launch_server \
  --model-path "$MODEL_DIR" \
  --served-model-name "$MODEL_NAME" \
  --tp 8 \
  --max-running-requests 256 \
  --watchdog-timeout 36000 \
  --trust-remote-code \
  --host 0.0.0.0 \
  --port 8000

---------------------------------------------------------------------
chat模型评测配置文件：

    _meta_template = dict(
        round=[
            dict(role='HUMAN', api_role='HUMAN', begin='', end=''),
            dict(role='BOT', api_role='BOT', begin='', end='', generate=True),
        ],
        begin='You are a helpful assistant.',
    )

    models = [
        dict(
            abbr='MODEL_NAME',
            type=opencompass.models.OpenAISDK,
            key='EMPTY',
            path="MODEL_NAME",
            openai_api_base='API_URL/v1',
            tokenizer_path='TOKENIZER_PATH',
            rpm_verbose=True,
            meta_template=_meta_template,
            stop_words=['<|end_of_text|>', '<|eot_id|>'],
            query_per_second=256,
            max_out_len=8192,
            max_seq_len=8192,
            temperature=0,
            batch_size=32,
            run_cfg=dict(num_gpus=0),
            retry=3,
            extra_body=dict(top_p=1, top_k=-1),
        )
    ]

    infer = dict(
        partitioner=dict(type=NumWorkerPartitioner, num_worker=8),
        runner=dict(
            type=LocalRunner, max_num_workers=1024,
            task=dict(type=OpenICLInferTask)),
    )

    eval = dict(
        partitioner=dict(type=NaivePartitioner),
        runner=dict(type=LocalRunner,
                max_num_workers=64,
                task=dict(type=OpenICLEvalTask)),
    )
