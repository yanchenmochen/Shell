MUSA_BLOCK_ARBITRATION_MODE            = -1            : Set the arbitration mode, -1: DEFAULT.  0: RoundRobin.  1: QueuePriority  2: KernelRoundRobin.  3: KernelQueuePriority
MUSA_BLOCK_DISTRIBUTION_GRANULARITY    = 0             : The block distribution granularity; 0: per mp(by default) or 1: per mpc.
MUSA_BLOCK_DISTRIBUTION_MODE           = 1             : The block distribution unit; 0: thread based or 1: block based(by default).
MUSA_BLOCK_SCHEDULE_MODE               = 1             : Set the schedule mode of kernel blocks, -1: DEFAULT.  0: DEMAND.  1: ROUND_ROBIN.  2: TASK_DEMAND.  3: DYNAMIC_BALANCED.
MUSA_BLOCK_STARTING                    = false         : Config block dispatching starts from 0: last ending by default or 1: always core0.
MUSA_CDM_PREFETCH                      = false         : Enable cdm prefetch.
MUSA_DEVICE_ORDER                      = FASTEST_FIRST : Enumerate all devices by compute capability (default:FASTEST_FIRST) or PCI Bus ID (PCI_BUS_ID) order.
MUSA_DUMP_DEVICE_BINARY                = false         : Dump device code object to file.
MUSA_DUMP_KERNEL_ASSEMBLY              = false         : Dump assembly to file named by kernel.
MUSA_EXECUTE_COUNT                     = 0             : Set the number of blocks dispatched to each core.
MUSA_EXECUTION_TIMEOUT                 = 3000000       : Specify kernel and memory operations execution timeout(ms), 200000ms by default.
MUSA_FORCE_SINGLE_CORE                 = false         : Force kernel execution on single core.
MUSA_FORCE_SINGLE_QUEUE                = false         : Force use single compute hardware queue; 0: use multiple compute queue if avaiable or 1: use single compute queue.
MUSA_INFLIGHT_SUBMISSION_LIMIT         = 0             : Config the limit of inflight submissions, The default value 0 gives control to driver.
MUSA_LAUNCH_BLOCKING                   = false         : Enable blocking kernel launches.
MUSA_LOG                               = 1             : Print API trace and debug logging. Bitmask (MUSA_LOG=0xffff)
MUSA_MANAGED_FORCE_DEVICE_ALLOC        = false         : Forces the driver to place all managed allocations in device memory.
MUSA_MEMCPY_PATH                       = 0             : Select mu/musaMemcpy copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_MEMSET_PATH                       = 0             : Select mu/musaMemset copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_MODULE_LOADING                    = DEFAULT       : Specify the module loading for the application.
MUSA_PRETEND_SUPPORT                   = false         : Return success rather than not supported for performance affected only APIs.
MUSA_PRINT_ENV                         = true          : Print MUSA environment variables.
MUSA_SEMAPHORE_OPEN_MODE               = 1             : The semaphore open mode; 0: mtlink first(fall back pcie if disenabled mtlink)  or 1: only pcie(by default).
MUSA_STREAM_ASYNC_CAPACITY             = 1024          : Config async command capacity of one stream, futher async api on the stream will be blocked. Default value is 1024.
MUSA_TRACK_COMMAND_TIMESTAMP           = false         : Track commands start and end timestamp.
MUSA_VIRTUAL_ALIGNMENT                 = 0x40000       : Specify memory mapping alignment, which will be clamped to the closest available page size.
MUSA_VISIBLE_DEVICES                   = 0,1,2,3,4,5,6,7 : Specify visible devices.
MUSA_ENABLE_COREDUMP_ON_EXCEPTION      = true          : Enable coreDump for exception.
MUSA_ENABLE_CPU_COREDUMP_ON_EXCEPTION  = false         : Triggers host (CPU) core dump after GPU core dump is complete. Enabled by default.
MUSA_ENABLE_LIGHTWEIGHT_COREDUMP       = false         : When enabled, GPU core dumps will not contain the memory dumps (local, shared, global) of the application. Disabled by default.
MUSA_ENABLE_USER_TRIGGERED_COREDUMP    = false         : Enables user triggerable core dumps by writing to a pipe defined in the COREDUMP_PIPE setting. Disabled by default.
MUSA_COREDUMP_FILE                     =               : Filename template for the GPU core dump.
MUSA_COREDUMP_PIPE                     =               : Filename template for the user pipe trigger.
MUSA_COREDUMP_GENERATION_FLAGS         =               : Flags used to control GPU coredump generation settings. 
MUSA_ENABLE_FP_EXCEPTION               = false         : Flags used to control whether floating-point exceptions (FP exceptions)  are detected and handled or reported by HW
I1111 20:27:38.111000 2864199 torch/distributed/launcher/api.py:194] Starting elastic_operator with launch configs:
I1111 20:27:38.111000 2864199 torch/distributed/launcher/api.py:194]   entrypoint       : pretrain_deepseekv2.py
I1111 20:27:38.111000 2864199 torch/distributed/launcher/api.py:194]   min_nodes        : 60
I1111 20:27:38.111000 2864199 torch/distributed/launcher/api.py:194]   max_nodes        : 60
I1111 20:27:38.111000 2864199 torch/distributed/launcher/api.py:194]   nproc_per_node   : 8
I1111 20:27:38.111000 2864199 torch/distributed/launcher/api.py:194]   run_id           : none
I1111 20:27:38.111000 2864199 torch/distributed/launcher/api.py:194]   rdzv_backend     : static
I1111 20:27:38.111000 2864199 torch/distributed/launcher/api.py:194]   rdzv_endpoint    : ji-aitrain-148214942333422912-master-0.ji-aitrain-148214942333422912:9958
I1111 20:27:38.111000 2864199 torch/distributed/launcher/api.py:194]   rdzv_configs     : {'rank': 27, 'timeout': 900}
I1111 20:27:38.111000 2864199 torch/distributed/launcher/api.py:194]   max_restarts     : 0
I1111 20:27:38.111000 2864199 torch/distributed/launcher/api.py:194]   monitor_interval : 0.1
I1111 20:27:38.111000 2864199 torch/distributed/launcher/api.py:194]   log_dir          : /tmp/torchelastic_fd9phi25
I1111 20:27:38.111000 2864199 torch/distributed/launcher/api.py:194]   metrics_cfg      : {}
I1111 20:27:38.111000 2864199 torch/distributed/launcher/api.py:194] 
I1111 20:27:38.112000 2864199 torch/distributed/elastic/agent/server/api.py:845] [default] starting workers for entrypoint: python
I1111 20:27:38.112000 2864199 torch/distributed/elastic/agent/server/api.py:662] [default] Rendezvous'ing worker group
I1111 20:27:40.415000 2864199 torch/distributed/elastic/agent/server/api.py:525] [default] Rendezvous complete for workers. Result:
I1111 20:27:40.415000 2864199 torch/distributed/elastic/agent/server/api.py:525]   restart_count=0
I1111 20:27:40.415000 2864199 torch/distributed/elastic/agent/server/api.py:525]   master_addr=ji-aitrain-148214942333422912-master-0.ji-aitrain-148214942333422912
I1111 20:27:40.415000 2864199 torch/distributed/elastic/agent/server/api.py:525]   master_port=9958
I1111 20:27:40.415000 2864199 torch/distributed/elastic/agent/server/api.py:525]   group_rank=27
I1111 20:27:40.415000 2864199 torch/distributed/elastic/agent/server/api.py:525]   group_world_size=60
I1111 20:27:40.415000 2864199 torch/distributed/elastic/agent/server/api.py:525]   local_ranks=[0, 1, 2, 3, 4, 5, 6, 7]
I1111 20:27:40.415000 2864199 torch/distributed/elastic/agent/server/api.py:525]   role_ranks=[216, 217, 218, 219, 220, 221, 222, 223]
I1111 20:27:40.415000 2864199 torch/distributed/elastic/agent/server/api.py:525]   global_ranks=[216, 217, 218, 219, 220, 221, 222, 223]
I1111 20:27:40.415000 2864199 torch/distributed/elastic/agent/server/api.py:525]   role_world_sizes=[480, 480, 480, 480, 480, 480, 480, 480]
I1111 20:27:40.415000 2864199 torch/distributed/elastic/agent/server/api.py:525]   global_world_sizes=[480, 480, 480, 480, 480, 480, 480, 480]
I1111 20:27:40.415000 2864199 torch/distributed/elastic/agent/server/api.py:525] 
I1111 20:27:40.416000 2864199 torch/distributed/elastic/agent/server/api.py:670] [default] Starting worker group
I1111 20:27:40.416000 2864199 torch/distributed/elastic/agent/server/local_elastic_agent.py:291] use_agent_store: True
I1111 20:27:40.448000 2864199 torch/distributed/elastic/agent/server/local_elastic_agent.py:192] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
I1111 20:27:40.448000 2864199 torch/distributed/elastic/agent/server/local_elastic_agent.py:229] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.
MUSA_BLOCK_ARBITRATION_MODE            = -1            : Set the arbitration mode, -1: DEFAULT.  0: RoundRobin.  1: QueuePriority  2: KernelRoundRobin.  3: KernelQueuePriority
MUSA_BLOCK_ARBITRATION_MODE            = -1            : Set the arbitration mode, -1: DEFAULT.  0: RoundRobin.  1: QueuePriority  2: KernelRoundRobin.  3: KernelQueuePriority
MUSA_BLOCK_ARBITRATION_MODE            = -1            : Set the arbitration mode, -1: DEFAULT.  0: RoundRobin.  1: QueuePriority  2: KernelRoundRobin.  3: KernelQueuePriority
MUSA_BLOCK_DISTRIBUTION_GRANULARITY    = 0             : The block distribution granularity; 0: per mp(by default) or 1: per mpc.
MUSA_BLOCK_DISTRIBUTION_GRANULARITY    = 0             : The block distribution granularity; 0: per mp(by default) or 1: per mpc.
MUSA_BLOCK_DISTRIBUTION_MODE           = 1             : The block distribution unit; 0: thread based or 1: block based(by default).
MUSA_BLOCK_DISTRIBUTION_GRANULARITY    = 0             : The block distribution granularity; 0: per mp(by default) or 1: per mpc.
MUSA_BLOCK_SCHEDULE_MODE               = 1             : Set the schedule mode of kernel blocks, -1: DEFAULT.  0: DEMAND.  1: ROUND_ROBIN.  2: TASK_DEMAND.  3: DYNAMIC_BALANCED.
MUSA_BLOCK_DISTRIBUTION_MODE           = 1             : The block distribution unit; 0: thread based or 1: block based(by default).
MUSA_BLOCK_DISTRIBUTION_MODE           = 1             : The block distribution unit; 0: thread based or 1: block based(by default).
MUSA_BLOCK_SCHEDULE_MODE               = 1             : Set the schedule mode of kernel blocks, -1: DEFAULT.  0: DEMAND.  1: ROUND_ROBIN.  2: TASK_DEMAND.  3: DYNAMIC_BALANCED.
MUSA_BLOCK_STARTING                    = false         : Config block dispatching starts from 0: last ending by default or 1: always core0.
MUSA_BLOCK_SCHEDULE_MODE               = 1             : Set the schedule mode of kernel blocks, -1: DEFAULT.  0: DEMAND.  1: ROUND_ROBIN.  2: TASK_DEMAND.  3: DYNAMIC_BALANCED.
MUSA_BLOCK_STARTING                    = false         : Config block dispatching starts from 0: last ending by default or 1: always core0.
MUSA_CDM_PREFETCH                      = false         : Enable cdm prefetch.
MUSA_CDM_PREFETCH                      = false         : Enable cdm prefetch.
MUSA_BLOCK_STARTING                    = false         : Config block dispatching starts from 0: last ending by default or 1: always core0.
MUSA_DEVICE_ORDER                      = FASTEST_FIRST : Enumerate all devices by compute capability (default:FASTEST_FIRST) or PCI Bus ID (PCI_BUS_ID) order.
MUSA_DEVICE_ORDER                      = FASTEST_FIRST : Enumerate all devices by compute capability (default:FASTEST_FIRST) or PCI Bus ID (PCI_BUS_ID) order.
MUSA_CDM_PREFETCH                      = false         : Enable cdm prefetch.
MUSA_DUMP_DEVICE_BINARY                = false         : Dump device code object to file.
MUSA_DUMP_DEVICE_BINARY                = false         : Dump device code object to file.
MUSA_DEVICE_ORDER                      = FASTEST_FIRST : Enumerate all devices by compute capability (default:FASTEST_FIRST) or PCI Bus ID (PCI_BUS_ID) order.
MUSA_DUMP_KERNEL_ASSEMBLY              = false         : Dump assembly to file named by kernel.
MUSA_DUMP_DEVICE_BINARY                = false         : Dump device code object to file.
MUSA_DUMP_KERNEL_ASSEMBLY              = false         : Dump assembly to file named by kernel.
MUSA_EXECUTE_COUNT                     = 0             : Set the number of blocks dispatched to each core.
MUSA_DUMP_KERNEL_ASSEMBLY              = false         : Dump assembly to file named by kernel.
MUSA_EXECUTE_COUNT                     = 0             : Set the number of blocks dispatched to each core.
MUSA_EXECUTION_TIMEOUT                 = 3000000       : Specify kernel and memory operations execution timeout(ms), 200000ms by default.
MUSA_EXECUTE_COUNT                     = 0             : Set the number of blocks dispatched to each core.
MUSA_FORCE_SINGLE_CORE                 = false         : Force kernel execution on single core.
MUSA_EXECUTION_TIMEOUT                 = 3000000       : Specify kernel and memory operations execution timeout(ms), 200000ms by default.
MUSA_EXECUTION_TIMEOUT                 = 3000000       : Specify kernel and memory operations execution timeout(ms), 200000ms by default.
MUSA_FORCE_SINGLE_QUEUE                = false         : Force use single compute hardware queue; 0: use multiple compute queue if avaiable or 1: use single compute queue.
MUSA_FORCE_SINGLE_CORE                 = false         : Force kernel execution on single core.
MUSA_FORCE_SINGLE_CORE                 = false         : Force kernel execution on single core.
MUSA_INFLIGHT_SUBMISSION_LIMIT         = 0             : Config the limit of inflight submissions, The default value 0 gives control to driver.
MUSA_FORCE_SINGLE_QUEUE                = false         : Force use single compute hardware queue; 0: use multiple compute queue if avaiable or 1: use single compute queue.
MUSA_FORCE_SINGLE_QUEUE                = false         : Force use single compute hardware queue; 0: use multiple compute queue if avaiable or 1: use single compute queue.
MUSA_LAUNCH_BLOCKING                   = false         : Enable blocking kernel launches.
MUSA_INFLIGHT_SUBMISSION_LIMIT         = 0             : Config the limit of inflight submissions, The default value 0 gives control to driver.
MUSA_INFLIGHT_SUBMISSION_LIMIT         = 0             : Config the limit of inflight submissions, The default value 0 gives control to driver.
MUSA_LOG                               = 1             : Print API trace and debug logging. Bitmask (MUSA_LOG=0xffff)
MUSA_LAUNCH_BLOCKING                   = false         : Enable blocking kernel launches.
MUSA_LAUNCH_BLOCKING                   = false         : Enable blocking kernel launches.
MUSA_MANAGED_FORCE_DEVICE_ALLOC        = false         : Forces the driver to place all managed allocations in device memory.
MUSA_LOG                               = 1             : Print API trace and debug logging. Bitmask (MUSA_LOG=0xffff)
MUSA_LOG                               = 1             : Print API trace and debug logging. Bitmask (MUSA_LOG=0xffff)
MUSA_MEMCPY_PATH                       = 0             : Select mu/musaMemcpy copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_MANAGED_FORCE_DEVICE_ALLOC        = false         : Forces the driver to place all managed allocations in device memory.
MUSA_MANAGED_FORCE_DEVICE_ALLOC        = false         : Forces the driver to place all managed allocations in device memory.
MUSA_MEMSET_PATH                       = 0             : Select mu/musaMemset copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_MEMCPY_PATH                       = 0             : Select mu/musaMemcpy copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_MEMCPY_PATH                       = 0             : Select mu/musaMemcpy copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_MODULE_LOADING                    = DEFAULT       : Specify the module loading for the application.
MUSA_MEMSET_PATH                       = 0             : Select mu/musaMemset copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_MEMSET_PATH                       = 0             : Select mu/musaMemset copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_PRETEND_SUPPORT                   = false         : Return success rather than not supported for performance affected only APIs.
MUSA_MODULE_LOADING                    = DEFAULT       : Specify the module loading for the application.
MUSA_MODULE_LOADING                    = DEFAULT       : Specify the module loading for the application.
MUSA_PRETEND_SUPPORT                   = false         : Return success rather than not supported for performance affected only APIs.
MUSA_PRETEND_SUPPORT                   = false         : Return success rather than not supported for performance affected only APIs.
MUSA_PRINT_ENV                         = true          : Print MUSA environment variables.
MUSA_SEMAPHORE_OPEN_MODE               = 1             : The semaphore open mode; 0: mtlink first(fall back pcie if disenabled mtlink)  or 1: only pcie(by default).
MUSA_STREAM_ASYNC_CAPACITY             = 1024          : Config async command capacity of one stream, futher async api on the stream will be blocked. Default value is 1024.
MUSA_PRINT_ENV                         = true          : Print MUSA environment variables.
MUSA_PRINT_ENV                         = true          : Print MUSA environment variables.
MUSA_TRACK_COMMAND_TIMESTAMP           = false         : Track commands start and end timestamp.
MUSA_SEMAPHORE_OPEN_MODE               = 1             : The semaphore open mode; 0: mtlink first(fall back pcie if disenabled mtlink)  or 1: only pcie(by default).
MUSA_SEMAPHORE_OPEN_MODE               = 1             : The semaphore open mode; 0: mtlink first(fall back pcie if disenabled mtlink)  or 1: only pcie(by default).
MUSA_VIRTUAL_ALIGNMENT                 = 0x40000       : Specify memory mapping alignment, which will be clamped to the closest available page size.
MUSA_STREAM_ASYNC_CAPACITY             = 1024          : Config async command capacity of one stream, futher async api on the stream will be blocked. Default value is 1024.
MUSA_STREAM_ASYNC_CAPACITY             = 1024          : Config async command capacity of one stream, futher async api on the stream will be blocked. Default value is 1024.
MUSA_TRACK_COMMAND_TIMESTAMP           = false         : Track commands start and end timestamp.
MUSA_TRACK_COMMAND_TIMESTAMP           = false         : Track commands start and end timestamp.
MUSA_VIRTUAL_ALIGNMENT                 = 0x40000       : Specify memory mapping alignment, which will be clamped to the closest available page size.
MUSA_VIRTUAL_ALIGNMENT                 = 0x40000       : Specify memory mapping alignment, which will be clamped to the closest available page size.
MUSA_VISIBLE_DEVICES                   = 0,1,2,3,4,5,6,7 : Specify visible devices.
MUSA_ENABLE_COREDUMP_ON_EXCEPTION      = true          : Enable coreDump for exception.
MUSA_ENABLE_CPU_COREDUMP_ON_EXCEPTION  = false         : Triggers host (CPU) core dump after GPU core dump is complete. Enabled by default.
MUSA_ENABLE_LIGHTWEIGHT_COREDUMP       = false         : When enabled, GPU core dumps will not contain the memory dumps (local, shared, global) of the application. Disabled by default.
MUSA_ENABLE_USER_TRIGGERED_COREDUMP    = false         : Enables user triggerable core dumps by writing to a pipe defined in the COREDUMP_PIPE setting. Disabled by default.
MUSA_COREDUMP_FILE                     =               : Filename template for the GPU core dump.
MUSA_COREDUMP_PIPE                     =               : Filename template for the user pipe trigger.
MUSA_COREDUMP_GENERATION_FLAGS         =               : Flags used to control GPU coredump generation settings. 
MUSA_VISIBLE_DEVICES                   = 0,1,2,3,4,5,6,7 : Specify visible devices.
MUSA_VISIBLE_DEVICES                   = 0,1,2,3,4,5,6,7 : Specify visible devices.
MUSA_ENABLE_FP_EXCEPTION               = false         : Flags used to control whether floating-point exceptions (FP exceptions)  are detected and handled or reported by HW
MUSA_ENABLE_COREDUMP_ON_EXCEPTION      = true          : Enable coreDump for exception.
MUSA_ENABLE_COREDUMP_ON_EXCEPTION      = true          : Enable coreDump for exception.
MUSA_ENABLE_CPU_COREDUMP_ON_EXCEPTION  = false         : Triggers host (CPU) core dump after GPU core dump is complete. Enabled by default.
MUSA_ENABLE_CPU_COREDUMP_ON_EXCEPTION  = false         : Triggers host (CPU) core dump after GPU core dump is complete. Enabled by default.
MUSA_ENABLE_LIGHTWEIGHT_COREDUMP       = false         : When enabled, GPU core dumps will not contain the memory dumps (local, shared, global) of the application. Disabled by default.
MUSA_ENABLE_LIGHTWEIGHT_COREDUMP       = false         : When enabled, GPU core dumps will not contain the memory dumps (local, shared, global) of the application. Disabled by default.
MUSA_ENABLE_USER_TRIGGERED_COREDUMP    = false         : Enables user triggerable core dumps by writing to a pipe defined in the COREDUMP_PIPE setting. Disabled by default.
MUSA_ENABLE_USER_TRIGGERED_COREDUMP    = false         : Enables user triggerable core dumps by writing to a pipe defined in the COREDUMP_PIPE setting. Disabled by default.
MUSA_COREDUMP_FILE                     =               : Filename template for the GPU core dump.
MUSA_COREDUMP_FILE                     =               : Filename template for the GPU core dump.
MUSA_COREDUMP_PIPE                     =               : Filename template for the user pipe trigger.
MUSA_COREDUMP_PIPE                     =               : Filename template for the user pipe trigger.
MUSA_COREDUMP_GENERATION_FLAGS         =               : Flags used to control GPU coredump generation settings. 
MUSA_COREDUMP_GENERATION_FLAGS         =               : Flags used to control GPU coredump generation settings. 
MUSA_ENABLE_FP_EXCEPTION               = false         : Flags used to control whether floating-point exceptions (FP exceptions)  are detected and handled or reported by HW
MUSA_ENABLE_FP_EXCEPTION               = false         : Flags used to control whether floating-point exceptions (FP exceptions)  are detected and handled or reported by HW
MUSA_BLOCK_ARBITRATION_MODE            = -1            : Set the arbitration mode, -1: DEFAULT.  0: RoundRobin.  1: QueuePriority  2: KernelRoundRobin.  3: KernelQueuePriority
MUSA_BLOCK_DISTRIBUTION_GRANULARITY    = 0             : The block distribution granularity; 0: per mp(by default) or 1: per mpc.
MUSA_BLOCK_DISTRIBUTION_MODE           = 1             : The block distribution unit; 0: thread based or 1: block based(by default).
MUSA_BLOCK_SCHEDULE_MODE               = 1             : Set the schedule mode of kernel blocks, -1: DEFAULT.  0: DEMAND.  1: ROUND_ROBIN.  2: TASK_DEMAND.  3: DYNAMIC_BALANCED.
MUSA_BLOCK_STARTING                    = false         : Config block dispatching starts from 0: last ending by default or 1: always core0.
MUSA_CDM_PREFETCH                      = false         : Enable cdm prefetch.
MUSA_DEVICE_ORDER                      = FASTEST_FIRST : Enumerate all devices by compute capability (default:FASTEST_FIRST) or PCI Bus ID (PCI_BUS_ID) order.
MUSA_DUMP_DEVICE_BINARY                = false         : Dump device code object to file.
MUSA_DUMP_KERNEL_ASSEMBLY              = false         : Dump assembly to file named by kernel.
MUSA_EXECUTE_COUNT                     = 0             : Set the number of blocks dispatched to each core.
MUSA_EXECUTION_TIMEOUT                 = 3000000       : Specify kernel and memory operations execution timeout(ms), 200000ms by default.
MUSA_FORCE_SINGLE_CORE                 = false         : Force kernel execution on single core.
MUSA_FORCE_SINGLE_QUEUE                = false         : Force use single compute hardware queue; 0: use multiple compute queue if avaiable or 1: use single compute queue.
MUSA_INFLIGHT_SUBMISSION_LIMIT         = 0             : Config the limit of inflight submissions, The default value 0 gives control to driver.
MUSA_LAUNCH_BLOCKING                   = false         : Enable blocking kernel launches.
MUSA_LOG                               = 1             : Print API trace and debug logging. Bitmask (MUSA_LOG=0xffff)
MUSA_MANAGED_FORCE_DEVICE_ALLOC        = false         : Forces the driver to place all managed allocations in device memory.
MUSA_MEMCPY_PATH                       = 0             : Select mu/musaMemcpy copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_MEMSET_PATH                       = 0             : Select mu/musaMemset copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_MODULE_LOADING                    = DEFAULT       : Specify the module loading for the application.
MUSA_PRETEND_SUPPORT                   = false         : Return success rather than not supported for performance affected only APIs.
MUSA_PRINT_ENV                         = true          : Print MUSA environment variables.
MUSA_SEMAPHORE_OPEN_MODE               = 1             : The semaphore open mode; 0: mtlink first(fall back pcie if disenabled mtlink)  or 1: only pcie(by default).
MUSA_STREAM_ASYNC_CAPACITY             = 1024          : Config async command capacity of one stream, futher async api on the stream will be blocked. Default value is 1024.
MUSA_TRACK_COMMAND_TIMESTAMP           = false         : Track commands start and end timestamp.
MUSA_VIRTUAL_ALIGNMENT                 = 0x40000       : Specify memory mapping alignment, which will be clamped to the closest available page size.
MUSA_VISIBLE_DEVICES                   = 0,1,2,3,4,5,6,7 : Specify visible devices.
MUSA_ENABLE_COREDUMP_ON_EXCEPTION      = true          : Enable coreDump for exception.
MUSA_ENABLE_CPU_COREDUMP_ON_EXCEPTION  = false         : Triggers host (CPU) core dump after GPU core dump is complete. Enabled by default.
MUSA_ENABLE_LIGHTWEIGHT_COREDUMP       = false         : When enabled, GPU core dumps will not contain the memory dumps (local, shared, global) of the application. Disabled by default.
MUSA_ENABLE_USER_TRIGGERED_COREDUMP    = false         : Enables user triggerable core dumps by writing to a pipe defined in the COREDUMP_PIPE setting. Disabled by default.
MUSA_COREDUMP_FILE                     =               : Filename template for the GPU core dump.
MUSA_COREDUMP_PIPE                     =               : Filename template for the user pipe trigger.
MUSA_COREDUMP_GENERATION_FLAGS         =               : Flags used to control GPU coredump generation settings. 
MUSA_ENABLE_FP_EXCEPTION               = false         : Flags used to control whether floating-point exceptions (FP exceptions)  are detected and handled or reported by HW
MUSA_BLOCK_ARBITRATION_MODE            = -1            : Set the arbitration mode, -1: DEFAULT.  0: RoundRobin.  1: QueuePriority  2: KernelRoundRobin.  3: KernelQueuePriority
MUSA_BLOCK_DISTRIBUTION_GRANULARITY    = 0             : The block distribution granularity; 0: per mp(by default) or 1: per mpc.
MUSA_BLOCK_DISTRIBUTION_MODE           = 1             : The block distribution unit; 0: thread based or 1: block based(by default).
MUSA_BLOCK_SCHEDULE_MODE               = 1             : Set the schedule mode of kernel blocks, -1: DEFAULT.  0: DEMAND.  1: ROUND_ROBIN.  2: TASK_DEMAND.  3: DYNAMIC_BALANCED.
MUSA_BLOCK_STARTING                    = false         : Config block dispatching starts from 0: last ending by default or 1: always core0.
MUSA_CDM_PREFETCH                      = false         : Enable cdm prefetch.
MUSA_DEVICE_ORDER                      = FASTEST_FIRST : Enumerate all devices by compute capability (default:FASTEST_FIRST) or PCI Bus ID (PCI_BUS_ID) order.
MUSA_DUMP_DEVICE_BINARY                = false         : Dump device code object to file.
MUSA_DUMP_KERNEL_ASSEMBLY              = false         : Dump assembly to file named by kernel.
MUSA_EXECUTE_COUNT                     = 0             : Set the number of blocks dispatched to each core.
MUSA_EXECUTION_TIMEOUT                 = 3000000       : Specify kernel and memory operations execution timeout(ms), 200000ms by default.
MUSA_FORCE_SINGLE_CORE                 = false         : Force kernel execution on single core.
MUSA_FORCE_SINGLE_QUEUE                = false         : Force use single compute hardware queue; 0: use multiple compute queue if avaiable or 1: use single compute queue.
MUSA_INFLIGHT_SUBMISSION_LIMIT         = 0             : Config the limit of inflight submissions, The default value 0 gives control to driver.
MUSA_LAUNCH_BLOCKING                   = false         : Enable blocking kernel launches.
MUSA_LOG                               = 1             : Print API trace and debug logging. Bitmask (MUSA_LOG=0xffff)
MUSA_MANAGED_FORCE_DEVICE_ALLOC        = false         : Forces the driver to place all managed allocations in device memory.
MUSA_MEMCPY_PATH                       = 0             : Select mu/musaMemcpy copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_MEMSET_PATH                       = 0             : Select mu/musaMemset copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_MODULE_LOADING                    = DEFAULT       : Specify the module loading for the application.
MUSA_PRETEND_SUPPORT                   = false         : Return success rather than not supported for performance affected only APIs.
MUSA_PRINT_ENV                         = true          : Print MUSA environment variables.
MUSA_SEMAPHORE_OPEN_MODE               = 1             : The semaphore open mode; 0: mtlink first(fall back pcie if disenabled mtlink)  or 1: only pcie(by default).
MUSA_STREAM_ASYNC_CAPACITY             = 1024          : Config async command capacity of one stream, futher async api on the stream will be blocked. Default value is 1024.
MUSA_TRACK_COMMAND_TIMESTAMP           = false         : Track commands start and end timestamp.
MUSA_VIRTUAL_ALIGNMENT                 = 0x40000       : Specify memory mapping alignment, which will be clamped to the closest available page size.
MUSA_VISIBLE_DEVICES                   = 0,1,2,3,4,5,6,7 : Specify visible devices.
MUSA_ENABLE_COREDUMP_ON_EXCEPTION      = true          : Enable coreDump for exception.
MUSA_ENABLE_CPU_COREDUMP_ON_EXCEPTION  = false         : Triggers host (CPU) core dump after GPU core dump is complete. Enabled by default.
MUSA_ENABLE_LIGHTWEIGHT_COREDUMP       = false         : When enabled, GPU core dumps will not contain the memory dumps (local, shared, global) of the application. Disabled by default.
MUSA_ENABLE_USER_TRIGGERED_COREDUMP    = false         : Enables user triggerable core dumps by writing to a pipe defined in the COREDUMP_PIPE setting. Disabled by default.
MUSA_COREDUMP_FILE                     =               : Filename template for the GPU core dump.
MUSA_COREDUMP_PIPE                     =               : Filename template for the user pipe trigger.
MUSA_COREDUMP_GENERATION_FLAGS         =               : Flags used to control GPU coredump generation settings. 
MUSA_ENABLE_FP_EXCEPTION               = false         : Flags used to control whether floating-point exceptions (FP exceptions)  are detected and handled or reported by HW
MUSA_BLOCK_ARBITRATION_MODE            = -1            : Set the arbitration mode, -1: DEFAULT.  0: RoundRobin.  1: QueuePriority  2: KernelRoundRobin.  3: KernelQueuePriority
MUSA_BLOCK_DISTRIBUTION_GRANULARITY    = 0             : The block distribution granularity; 0: per mp(by default) or 1: per mpc.
MUSA_BLOCK_DISTRIBUTION_MODE           = 1             : The block distribution unit; 0: thread based or 1: block based(by default).
MUSA_BLOCK_SCHEDULE_MODE               = 1             : Set the schedule mode of kernel blocks, -1: DEFAULT.  0: DEMAND.  1: ROUND_ROBIN.  2: TASK_DEMAND.  3: DYNAMIC_BALANCED.
MUSA_BLOCK_STARTING                    = false         : Config block dispatching starts from 0: last ending by default or 1: always core0.
MUSA_CDM_PREFETCH                      = false         : Enable cdm prefetch.
MUSA_DEVICE_ORDER                      = FASTEST_FIRST : Enumerate all devices by compute capability (default:FASTEST_FIRST) or PCI Bus ID (PCI_BUS_ID) order.
MUSA_DUMP_DEVICE_BINARY                = false         : Dump device code object to file.
MUSA_DUMP_KERNEL_ASSEMBLY              = false         : Dump assembly to file named by kernel.
MUSA_EXECUTE_COUNT                     = 0             : Set the number of blocks dispatched to each core.
MUSA_EXECUTION_TIMEOUT                 = 3000000       : Specify kernel and memory operations execution timeout(ms), 200000ms by default.
MUSA_FORCE_SINGLE_CORE                 = false         : Force kernel execution on single core.
MUSA_FORCE_SINGLE_QUEUE                = false         : Force use single compute hardware queue; 0: use multiple compute queue if avaiable or 1: use single compute queue.
MUSA_INFLIGHT_SUBMISSION_LIMIT         = 0             : Config the limit of inflight submissions, The default value 0 gives control to driver.
MUSA_LAUNCH_BLOCKING                   = false         : Enable blocking kernel launches.
MUSA_LOG                               = 1             : Print API trace and debug logging. Bitmask (MUSA_LOG=0xffff)
MUSA_MANAGED_FORCE_DEVICE_ALLOC        = false         : Forces the driver to place all managed allocations in device memory.
MUSA_MEMCPY_PATH                       = 0             : Select mu/musaMemcpy copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_MEMSET_PATH                       = 0             : Select mu/musaMemset copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_MODULE_LOADING                    = DEFAULT       : Specify the module loading for the application.
MUSA_PRETEND_SUPPORT                   = false         : Return success rather than not supported for performance affected only APIs.
MUSA_PRINT_ENV                         = true          : Print MUSA environment variables.
MUSA_SEMAPHORE_OPEN_MODE               = 1             : The semaphore open mode; 0: mtlink first(fall back pcie if disenabled mtlink)  or 1: only pcie(by default).
MUSA_STREAM_ASYNC_CAPACITY             = 1024          : Config async command capacity of one stream, futher async api on the stream will be blocked. Default value is 1024.
MUSA_TRACK_COMMAND_TIMESTAMP           = false         : Track commands start and end timestamp.
MUSA_VIRTUAL_ALIGNMENT                 = 0x40000       : Specify memory mapping alignment, which will be clamped to the closest available page size.
MUSA_VISIBLE_DEVICES                   = 0,1,2,3,4,5,6,7 : Specify visible devices.
MUSA_ENABLE_COREDUMP_ON_EXCEPTION      = true          : Enable coreDump for exception.
MUSA_ENABLE_CPU_COREDUMP_ON_EXCEPTION  = false         : Triggers host (CPU) core dump after GPU core dump is complete. Enabled by default.
MUSA_ENABLE_LIGHTWEIGHT_COREDUMP       = false         : When enabled, GPU core dumps will not contain the memory dumps (local, shared, global) of the application. Disabled by default.
MUSA_ENABLE_USER_TRIGGERED_COREDUMP    = false         : Enables user triggerable core dumps by writing to a pipe defined in the COREDUMP_PIPE setting. Disabled by default.
MUSA_COREDUMP_FILE                     =               : Filename template for the GPU core dump.
MUSA_COREDUMP_PIPE                     =               : Filename template for the user pipe trigger.
MUSA_COREDUMP_GENERATION_FLAGS         =               : Flags used to control GPU coredump generation settings. 
MUSA_ENABLE_FP_EXCEPTION               = false         : Flags used to control whether floating-point exceptions (FP exceptions)  are detected and handled or reported by HW
MUSA_BLOCK_ARBITRATION_MODE            = -1            : Set the arbitration mode, -1: DEFAULT.  0: RoundRobin.  1: QueuePriority  2: KernelRoundRobin.  3: KernelQueuePriority
MUSA_BLOCK_DISTRIBUTION_GRANULARITY    = 0             : The block distribution granularity; 0: per mp(by default) or 1: per mpc.
MUSA_BLOCK_DISTRIBUTION_MODE           = 1             : The block distribution unit; 0: thread based or 1: block based(by default).
MUSA_BLOCK_SCHEDULE_MODE               = 1             : Set the schedule mode of kernel blocks, -1: DEFAULT.  0: DEMAND.  1: ROUND_ROBIN.  2: TASK_DEMAND.  3: DYNAMIC_BALANCED.
MUSA_BLOCK_STARTING                    = false         : Config block dispatching starts from 0: last ending by default or 1: always core0.
MUSA_CDM_PREFETCH                      = false         : Enable cdm prefetch.
MUSA_DEVICE_ORDER                      = FASTEST_FIRST : Enumerate all devices by compute capability (default:FASTEST_FIRST) or PCI Bus ID (PCI_BUS_ID) order.
MUSA_DUMP_DEVICE_BINARY                = false         : Dump device code object to file.
MUSA_DUMP_KERNEL_ASSEMBLY              = false         : Dump assembly to file named by kernel.
MUSA_EXECUTE_COUNT                     = 0             : Set the number of blocks dispatched to each core.
MUSA_EXECUTION_TIMEOUT                 = 3000000       : Specify kernel and memory operations execution timeout(ms), 200000ms by default.
MUSA_FORCE_SINGLE_CORE                 = false         : Force kernel execution on single core.
MUSA_FORCE_SINGLE_QUEUE                = false         : Force use single compute hardware queue; 0: use multiple compute queue if avaiable or 1: use single compute queue.
MUSA_INFLIGHT_SUBMISSION_LIMIT         = 0             : Config the limit of inflight submissions, The default value 0 gives control to driver.
MUSA_LAUNCH_BLOCKING                   = false         : Enable blocking kernel launches.
MUSA_LOG                               = 1             : Print API trace and debug logging. Bitmask (MUSA_LOG=0xffff)
MUSA_MANAGED_FORCE_DEVICE_ALLOC        = false         : Forces the driver to place all managed allocations in device memory.
MUSA_MEMCPY_PATH                       = 0             : Select mu/musaMemcpy copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_MEMSET_PATH                       = 0             : Select mu/musaMemset copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_MODULE_LOADING                    = DEFAULT       : Specify the module loading for the application.
MUSA_PRETEND_SUPPORT                   = false         : Return success rather than not supported for performance affected only APIs.
MUSA_PRINT_ENV                         = true          : Print MUSA environment variables.
MUSA_SEMAPHORE_OPEN_MODE               = 1             : The semaphore open mode; 0: mtlink first(fall back pcie if disenabled mtlink)  or 1: only pcie(by default).
MUSA_STREAM_ASYNC_CAPACITY             = 1024          : Config async command capacity of one stream, futher async api on the stream will be blocked. Default value is 1024.
MUSA_TRACK_COMMAND_TIMESTAMP           = false         : Track commands start and end timestamp.
MUSA_VIRTUAL_ALIGNMENT                 = 0x40000       : Specify memory mapping alignment, which will be clamped to the closest available page size.
MUSA_VISIBLE_DEVICES                   = 0,1,2,3,4,5,6,7 : Specify visible devices.
MUSA_ENABLE_COREDUMP_ON_EXCEPTION      = true          : Enable coreDump for exception.
MUSA_ENABLE_CPU_COREDUMP_ON_EXCEPTION  = false         : Triggers host (CPU) core dump after GPU core dump is complete. Enabled by default.
MUSA_ENABLE_LIGHTWEIGHT_COREDUMP       = false         : When enabled, GPU core dumps will not contain the memory dumps (local, shared, global) of the application. Disabled by default.
MUSA_ENABLE_USER_TRIGGERED_COREDUMP    = false         : Enables user triggerable core dumps by writing to a pipe defined in the COREDUMP_PIPE setting. Disabled by default.
MUSA_COREDUMP_FILE                     =               : Filename template for the GPU core dump.
MUSA_COREDUMP_PIPE                     =               : Filename template for the user pipe trigger.
MUSA_COREDUMP_GENERATION_FLAGS         =               : Flags used to control GPU coredump generation settings. 
MUSA_ENABLE_FP_EXCEPTION               = false         : Flags used to control whether floating-point exceptions (FP exceptions)  are detected and handled or reported by HW
MUSA_BLOCK_ARBITRATION_MODE            = -1            : Set the arbitration mode, -1: DEFAULT.  0: RoundRobin.  1: QueuePriority  2: KernelRoundRobin.  3: KernelQueuePriority
MUSA_BLOCK_DISTRIBUTION_GRANULARITY    = 0             : The block distribution granularity; 0: per mp(by default) or 1: per mpc.
MUSA_BLOCK_DISTRIBUTION_MODE           = 1             : The block distribution unit; 0: thread based or 1: block based(by default).
MUSA_BLOCK_SCHEDULE_MODE               = 1             : Set the schedule mode of kernel blocks, -1: DEFAULT.  0: DEMAND.  1: ROUND_ROBIN.  2: TASK_DEMAND.  3: DYNAMIC_BALANCED.
MUSA_BLOCK_STARTING                    = false         : Config block dispatching starts from 0: last ending by default or 1: always core0.
MUSA_CDM_PREFETCH                      = false         : Enable cdm prefetch.
MUSA_DEVICE_ORDER                      = FASTEST_FIRST : Enumerate all devices by compute capability (default:FASTEST_FIRST) or PCI Bus ID (PCI_BUS_ID) order.
MUSA_DUMP_DEVICE_BINARY                = false         : Dump device code object to file.
MUSA_DUMP_KERNEL_ASSEMBLY              = false         : Dump assembly to file named by kernel.
MUSA_EXECUTE_COUNT                     = 0             : Set the number of blocks dispatched to each core.
MUSA_EXECUTION_TIMEOUT                 = 3000000       : Specify kernel and memory operations execution timeout(ms), 200000ms by default.
MUSA_FORCE_SINGLE_CORE                 = false         : Force kernel execution on single core.
MUSA_FORCE_SINGLE_QUEUE                = false         : Force use single compute hardware queue; 0: use multiple compute queue if avaiable or 1: use single compute queue.
MUSA_INFLIGHT_SUBMISSION_LIMIT         = 0             : Config the limit of inflight submissions, The default value 0 gives control to driver.
MUSA_LAUNCH_BLOCKING                   = false         : Enable blocking kernel launches.
MUSA_LOG                               = 1             : Print API trace and debug logging. Bitmask (MUSA_LOG=0xffff)
MUSA_MANAGED_FORCE_DEVICE_ALLOC        = false         : Forces the driver to place all managed allocations in device memory.
MUSA_MEMCPY_PATH                       = 0             : Select mu/musaMemcpy copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_MEMSET_PATH                       = 0             : Select mu/musaMemset copyManager, 0: Default  1: DMA.  2: TDM.  3: CE.  4: CPU.  5:CDM.  6:CDMShaderCopy.
MUSA_MODULE_LOADING                    = DEFAULT       : Specify the module loading for the application.
MUSA_PRETEND_SUPPORT                   = false         : Return success rather than not supported for performance affected only APIs.
MUSA_PRINT_ENV                         = true          : Print MUSA environment variables.
MUSA_SEMAPHORE_OPEN_MODE               = 1             : The semaphore open mode; 0: mtlink first(fall back pcie if disenabled mtlink)  or 1: only pcie(by default).
MUSA_STREAM_ASYNC_CAPACITY             = 1024          : Config async command capacity of one stream, futher async api on the stream will be blocked. Default value is 1024.
MUSA_TRACK_COMMAND_TIMESTAMP           = false         : Track commands start and end timestamp.
MUSA_VIRTUAL_ALIGNMENT                 = 0x40000       : Specify memory mapping alignment, which will be clamped to the closest available page size.
MUSA_VISIBLE_DEVICES                   = 0,1,2,3,4,5,6,7 : Specify visible devices.
MUSA_ENABLE_COREDUMP_ON_EXCEPTION      = true          : Enable coreDump for exception.
MUSA_ENABLE_CPU_COREDUMP_ON_EXCEPTION  = false         : Triggers host (CPU) core dump after GPU core dump is complete. Enabled by default.
MUSA_ENABLE_LIGHTWEIGHT_COREDUMP       = false         : When enabled, GPU core dumps will not contain the memory dumps (local, shared, global) of the application. Disabled by default.
MUSA_ENABLE_USER_TRIGGERED_COREDUMP    = false         : Enables user triggerable core dumps by writing to a pipe defined in the COREDUMP_PIPE setting. Disabled by default.
MUSA_COREDUMP_FILE                     =               : Filename template for the GPU core dump.
MUSA_COREDUMP_PIPE                     =               : Filename template for the user pipe trigger.
MUSA_COREDUMP_GENERATION_FLAGS         =               : Flags used to control GPU coredump generation settings. 
MUSA_ENABLE_FP_EXCEPTION               = false         : Flags used to control whether floating-point exceptions (FP exceptions)  are detected and handled or reported by HW
/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/linear_with_grad_accumulation_and_async_allreduce.py:32: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/linear_with_grad_accumulation_and_async_allreduce.py:32: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/linear_with_grad_accumulation_and_async_allreduce.py:79: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/linear_with_grad_accumulation_and_async_allreduce.py:79: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/linear_with_grad_accumulation_and_async_allreduce.py:32: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/linear_with_grad_accumulation_and_async_allreduce.py:79: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/linear_with_grad_accumulation_and_async_allreduce.py:32: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/linear_with_grad_accumulation_and_async_allreduce.py:79: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/linear_with_grad_accumulation_and_async_allreduce.py:32: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/linear_with_grad_accumulation_and_async_allreduce.py:79: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/linear_with_grad_accumulation_and_async_allreduce.py:32: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/linear_with_grad_accumulation_and_async_allreduce.py:79: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/linear_with_grad_accumulation_and_async_allreduce.py:32: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/linear_with_grad_accumulation_and_async_allreduce.py:79: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/linear_with_grad_accumulation_and_async_allreduce.py:32: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/linear_with_grad_accumulation_and_async_allreduce.py:79: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
config_instance is config_instance is   config_instance is  config_instance is  config_instance is  config_instance is  config_instance is  config_instance is  MLATransformerConfig(tensor_model_parallel_size=1, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=1, virtual_pipeline_model_parallel_size=None, sequence_parallel=False, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=4, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=None, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=True, async_tensor_model_parallel_allreduce=True, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=False, overlap_p2p_comm=False, batch_p2p_comm=True, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=True, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=1, delay_wgrad_compute=False, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=28, mtp_num_layers=None, mtp_loss_scaling_factor=0.1, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, pipeline_model_parallel_layout=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=2048, num_attention_heads=16, attention_backend=<AttnBackend.auto: 5>, softmax_scale=None, num_query_groups=16, ffn_hidden_size=10944, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0x7fa454ed8c10>, activation_func_fp8_input_store=False, num_moe_experts=64, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=True, no_rope_freq=None, moe_deepep_num_sms=20, init_method=functools.partial(<function normal_ at 0x7fa454d71480>, mean=0.0, std=0.006), output_layer_init_method=functools.partial(<function normal_ at 0x7fa454d71480>, mean=0.0, std=0.0008017837257372732), init_method_std=0.006, embedding_init_method=functools.partial(<function normal_ at 0x7fa454d71480>, mean=0.0, std=0.006), embedding_init_method_std=0.006, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=True, masked_softmax_fusion=False, persist_layer_norm=True, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=False, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=True, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, use_kitchen=False, moe_shared_expert_intermediate_size=2816, moe_shared_expert_overlap=False, moe_layer_freq=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], moe_ffn_hidden_size=1408, moe_router_load_balancing_type='aux_loss', moe_router_topk=6, moe_router_topk_limited_devices=None, moe_router_padding_for_fp8=False, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=True, moe_router_topk_scaling_factor=2.446, moe_router_score_function='softmax', moe_router_dtype='fp32', moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_router_force_load_balancing=False, moe_grouped_gemm=True, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0.001, moe_z_loss_coeff=0.0001, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=True, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type='p2p', enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, symmetric_ar_type=None, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, mamba_num_heads=None, use_mamba_mem_eff_path=True, mlp_chunks_for_prefill=1, heterogeneous_block_specs=False, hetereogenous_dist_checkpoint=False, quant_recipe=None, q_lora_rank=None, kv_lora_rank=512, qk_head_dim=128, qk_pos_emb_head_dim=64, v_head_dim=128, rope_type='yarn', rotary_base=10000, rotary_percent=1.0, rotary_scaling_factor=1.0, max_position_embeddings=4096, original_max_position_embeddings=4096, beta_fast=32, beta_slow=1, mscale=1.0, mscale_all_dim=1.0)
MLATransformerConfig(tensor_model_parallel_size=1, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=1, virtual_pipeline_model_parallel_size=None, sequence_parallel=False, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=4, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=None, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=True, async_tensor_model_parallel_allreduce=True, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=False, overlap_p2p_comm=False, batch_p2p_comm=True, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=True, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=1, delay_wgrad_compute=False, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=28, mtp_num_layers=None, mtp_loss_scaling_factor=0.1, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, pipeline_model_parallel_layout=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=2048, num_attention_heads=16, attention_backend=<AttnBackend.auto: 5>, softmax_scale=None, num_query_groups=16, ffn_hidden_size=10944, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0x7f36795d0c10>, activation_func_fp8_input_store=False, num_moe_experts=64, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=True, no_rope_freq=None, moe_deepep_num_sms=20, init_method=functools.partial(<function normal_ at 0x7f3679465480>, mean=0.0, std=0.006), output_layer_init_method=functools.partial(<function normal_ at 0x7f3679465480>, mean=0.0, std=0.0008017837257372732), init_method_std=0.006, embedding_init_method=functools.partial(<function normal_ at 0x7f3679465480>, mean=0.0, std=0.006), embedding_init_method_std=0.006, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=True, masked_softmax_fusion=False, persist_layer_norm=True, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=False, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=True, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, use_kitchen=False, moe_shared_expert_intermediate_size=2816, moe_shared_expert_overlap=False, moe_layer_freq=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], moe_ffn_hidden_size=1408, moe_router_load_balancing_type='aux_loss', moe_router_topk=6, moe_router_topk_limited_devices=None, moe_router_padding_for_fp8=False, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=True, moe_router_topk_scaling_factor=2.446, moe_router_score_function='softmax', moe_router_dtype='fp32', moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_router_force_load_balancing=False, moe_grouped_gemm=True, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0.001, moe_z_loss_coeff=0.0001, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=True, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type='p2p', enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, symmetric_ar_type=None, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, mamba_num_heads=None, use_mamba_mem_eff_path=True, mlp_chunks_for_prefill=1, heterogeneous_block_specs=False, hetereogenous_dist_checkpoint=False, quant_recipe=None, q_lora_rank=None, kv_lora_rank=512, qk_head_dim=128, qk_pos_emb_head_dim=64, v_head_dim=128, rope_type='yarn', rotary_base=10000, rotary_percent=1.0, rotary_scaling_factor=1.0, max_position_embeddings=4096, original_max_position_embeddings=4096, beta_fast=32, beta_slow=1, mscale=1.0, mscale_all_dim=1.0)MLATransformerConfig(tensor_model_parallel_size=1, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=1, virtual_pipeline_model_parallel_size=None, sequence_parallel=False, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=4, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=None, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=True, async_tensor_model_parallel_allreduce=True, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=False, overlap_p2p_comm=False, batch_p2p_comm=True, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=True, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=1, delay_wgrad_compute=False, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=28, mtp_num_layers=None, mtp_loss_scaling_factor=0.1, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, pipeline_model_parallel_layout=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=2048, num_attention_heads=16, attention_backend=<AttnBackend.auto: 5>, softmax_scale=None, num_query_groups=16, ffn_hidden_size=10944, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0x7f7ef2c9cc10>, activation_func_fp8_input_store=False, num_moe_experts=64, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=True, no_rope_freq=None, moe_deepep_num_sms=20, init_method=functools.partial(<function normal_ at 0x7f7ef2b31480>, mean=0.0, std=0.006), output_layer_init_method=functools.partial(<function normal_ at 0x7f7ef2b31480>, mean=0.0, std=0.0008017837257372732), init_method_std=0.006, embedding_init_method=functools.partial(<function normal_ at 0x7f7ef2b31480>, mean=0.0, std=0.006), embedding_init_method_std=0.006, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=True, masked_softmax_fusion=False, persist_layer_norm=True, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=False, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=True, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, use_kitchen=False, moe_shared_expert_intermediate_size=2816, moe_shared_expert_overlap=False, moe_layer_freq=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], moe_ffn_hidden_size=1408, moe_router_load_balancing_type='aux_loss', moe_router_topk=6, moe_router_topk_limited_devices=None, moe_router_padding_for_fp8=False, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=True, moe_router_topk_scaling_factor=2.446, moe_router_score_function='softmax', moe_router_dtype='fp32', moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_router_force_load_balancing=False, moe_grouped_gemm=True, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0.001, moe_z_loss_coeff=0.0001, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=True, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type='p2p', enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, symmetric_ar_type=None, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, mamba_num_heads=None, use_mamba_mem_eff_path=True, mlp_chunks_for_prefill=1, heterogeneous_block_specs=False, hetereogenous_dist_checkpoint=False, quant_recipe=None, q_lora_rank=None, kv_lora_rank=512, qk_head_dim=128, qk_pos_emb_head_dim=64, v_head_dim=128, rope_type='yarn', rotary_base=10000, rotary_percent=1.0, rotary_scaling_factor=1.0, max_position_embeddings=4096, original_max_position_embeddings=4096, beta_fast=32, beta_slow=1, mscale=1.0, mscale_all_dim=1.0)MLATransformerConfig(tensor_model_parallel_size=1, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=1, virtual_pipeline_model_parallel_size=None, sequence_parallel=False, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=4, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=None, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=True, async_tensor_model_parallel_allreduce=True, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=False, overlap_p2p_comm=False, batch_p2p_comm=True, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=True, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=1, delay_wgrad_compute=False, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=28, mtp_num_layers=None, mtp_loss_scaling_factor=0.1, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, pipeline_model_parallel_layout=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=2048, num_attention_heads=16, attention_backend=<AttnBackend.auto: 5>, softmax_scale=None, num_query_groups=16, ffn_hidden_size=10944, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0x7f55e42a4c10>, activation_func_fp8_input_store=False, num_moe_experts=64, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=True, no_rope_freq=None, moe_deepep_num_sms=20, init_method=functools.partial(<function normal_ at 0x7f55e4139480>, mean=0.0, std=0.006), output_layer_init_method=functools.partial(<function normal_ at 0x7f55e4139480>, mean=0.0, std=0.0008017837257372732), init_method_std=0.006, embedding_init_method=functools.partial(<function normal_ at 0x7f55e4139480>, mean=0.0, std=0.006), embedding_init_method_std=0.006, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=True, masked_softmax_fusion=False, persist_layer_norm=True, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=False, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=True, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, use_kitchen=False, moe_shared_expert_intermediate_size=2816, moe_shared_expert_overlap=False, moe_layer_freq=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], moe_ffn_hidden_size=1408, moe_router_load_balancing_type='aux_loss', moe_router_topk=6, moe_router_topk_limited_devices=None, moe_router_padding_for_fp8=False, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=True, moe_router_topk_scaling_factor=2.446, moe_router_score_function='softmax', moe_router_dtype='fp32', moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_router_force_load_balancing=False, moe_grouped_gemm=True, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0.001, moe_z_loss_coeff=0.0001, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=True, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type='p2p', enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, symmetric_ar_type=None, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, mamba_num_heads=None, use_mamba_mem_eff_path=True, mlp_chunks_for_prefill=1, heterogeneous_block_specs=False, hetereogenous_dist_checkpoint=False, quant_recipe=None, q_lora_rank=None, kv_lora_rank=512, qk_head_dim=128, qk_pos_emb_head_dim=64, v_head_dim=128, rope_type='yarn', rotary_base=10000, rotary_percent=1.0, rotary_scaling_factor=1.0, max_position_embeddings=4096, original_max_position_embeddings=4096, beta_fast=32, beta_slow=1, mscale=1.0, mscale_all_dim=1.0)


MLATransformerConfig(tensor_model_parallel_size=1, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=1, virtual_pipeline_model_parallel_size=None, sequence_parallel=False, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=4, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=None, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=True, async_tensor_model_parallel_allreduce=True, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=False, overlap_p2p_comm=False, batch_p2p_comm=True, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=True, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=1, delay_wgrad_compute=False, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=28, mtp_num_layers=None, mtp_loss_scaling_factor=0.1, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, pipeline_model_parallel_layout=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=2048, num_attention_heads=16, attention_backend=<AttnBackend.auto: 5>, softmax_scale=None, num_query_groups=16, ffn_hidden_size=10944, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0x7f9442fe0c10>, activation_func_fp8_input_store=False, num_moe_experts=64, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=True, no_rope_freq=None, moe_deepep_num_sms=20, init_method=functools.partial(<function normal_ at 0x7f9442e75480>, mean=0.0, std=0.006), output_layer_init_method=functools.partial(<function normal_ at 0x7f9442e75480>, mean=0.0, std=0.0008017837257372732), init_method_std=0.006, embedding_init_method=functools.partial(<function normal_ at 0x7f9442e75480>, mean=0.0, std=0.006), embedding_init_method_std=0.006, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=True, masked_softmax_fusion=False, persist_layer_norm=True, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=False, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=True, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, use_kitchen=False, moe_shared_expert_intermediate_size=2816, moe_shared_expert_overlap=False, moe_layer_freq=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], moe_ffn_hidden_size=1408, moe_router_load_balancing_type='aux_loss', moe_router_topk=6, moe_router_topk_limited_devices=None, moe_router_padding_for_fp8=False, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=True, moe_router_topk_scaling_factor=2.446, moe_router_score_function='softmax', moe_router_dtype='fp32', moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_router_force_load_balancing=False, moe_grouped_gemm=True, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0.001, moe_z_loss_coeff=0.0001, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=True, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type='p2p', enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, symmetric_ar_type=None, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, mamba_num_heads=None, use_mamba_mem_eff_path=True, mlp_chunks_for_prefill=1, heterogeneous_block_specs=False, hetereogenous_dist_checkpoint=False, quant_recipe=None, q_lora_rank=None, kv_lora_rank=512, qk_head_dim=128, qk_pos_emb_head_dim=64, v_head_dim=128, rope_type='yarn', rotary_base=10000, rotary_percent=1.0, rotary_scaling_factor=1.0, max_position_embeddings=4096, original_max_position_embeddings=4096, beta_fast=32, beta_slow=1, mscale=1.0, mscale_all_dim=1.0)
MLATransformerConfig(tensor_model_parallel_size=1, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=1, virtual_pipeline_model_parallel_size=None, sequence_parallel=False, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=4, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=None, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=True, async_tensor_model_parallel_allreduce=True, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=False, overlap_p2p_comm=False, batch_p2p_comm=True, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=True, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=1, delay_wgrad_compute=False, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=28, mtp_num_layers=None, mtp_loss_scaling_factor=0.1, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, pipeline_model_parallel_layout=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=2048, num_attention_heads=16, attention_backend=<AttnBackend.auto: 5>, softmax_scale=None, num_query_groups=16, ffn_hidden_size=10944, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0x7f492f728c10>, activation_func_fp8_input_store=False, num_moe_experts=64, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=True, no_rope_freq=None, moe_deepep_num_sms=20, init_method=functools.partial(<function normal_ at 0x7f492f5bd480>, mean=0.0, std=0.006), output_layer_init_method=functools.partial(<function normal_ at 0x7f492f5bd480>, mean=0.0, std=0.0008017837257372732), init_method_std=0.006, embedding_init_method=functools.partial(<function normal_ at 0x7f492f5bd480>, mean=0.0, std=0.006), embedding_init_method_std=0.006, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=True, masked_softmax_fusion=False, persist_layer_norm=True, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=False, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=True, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, use_kitchen=False, moe_shared_expert_intermediate_size=2816, moe_shared_expert_overlap=False, moe_layer_freq=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], moe_ffn_hidden_size=1408, moe_router_load_balancing_type='aux_loss', moe_router_topk=6, moe_router_topk_limited_devices=None, moe_router_padding_for_fp8=False, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=True, moe_router_topk_scaling_factor=2.446, moe_router_score_function='softmax', moe_router_dtype='fp32', moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_router_force_load_balancing=False, moe_grouped_gemm=True, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0.001, moe_z_loss_coeff=0.0001, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=True, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type='p2p', enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, symmetric_ar_type=None, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, mamba_num_heads=None, use_mamba_mem_eff_path=True, mlp_chunks_for_prefill=1, heterogeneous_block_specs=False, hetereogenous_dist_checkpoint=False, quant_recipe=None, q_lora_rank=None, kv_lora_rank=512, qk_head_dim=128, qk_pos_emb_head_dim=64, v_head_dim=128, rope_type='yarn', rotary_base=10000, rotary_percent=1.0, rotary_scaling_factor=1.0, max_position_embeddings=4096, original_max_position_embeddings=4096, beta_fast=32, beta_slow=1, mscale=1.0, mscale_all_dim=1.0)
MLATransformerConfig(tensor_model_parallel_size=1, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=1, virtual_pipeline_model_parallel_size=None, sequence_parallel=False, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=4, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=None, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=True, async_tensor_model_parallel_allreduce=True, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=False, overlap_p2p_comm=False, batch_p2p_comm=True, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=True, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=1, delay_wgrad_compute=False, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=28, mtp_num_layers=None, mtp_loss_scaling_factor=0.1, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, pipeline_model_parallel_layout=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=2048, num_attention_heads=16, attention_backend=<AttnBackend.auto: 5>, softmax_scale=None, num_query_groups=16, ffn_hidden_size=10944, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0x7fe0de268c10>, activation_func_fp8_input_store=False, num_moe_experts=64, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=True, no_rope_freq=None, moe_deepep_num_sms=20, init_method=functools.partial(<function normal_ at 0x7fe0de0fd480>, mean=0.0, std=0.006), output_layer_init_method=functools.partial(<function normal_ at 0x7fe0de0fd480>, mean=0.0, std=0.0008017837257372732), init_method_std=0.006, embedding_init_method=functools.partial(<function normal_ at 0x7fe0de0fd480>, mean=0.0, std=0.006), embedding_init_method_std=0.006, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=True, masked_softmax_fusion=False, persist_layer_norm=True, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=False, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=True, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, use_kitchen=False, moe_shared_expert_intermediate_size=2816, moe_shared_expert_overlap=False, moe_layer_freq=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], moe_ffn_hidden_size=1408, moe_router_load_balancing_type='aux_loss', moe_router_topk=6, moe_router_topk_limited_devices=None, moe_router_padding_for_fp8=False, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=True, moe_router_topk_scaling_factor=2.446, moe_router_score_function='softmax', moe_router_dtype='fp32', moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_router_force_load_balancing=False, moe_grouped_gemm=True, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0.001, moe_z_loss_coeff=0.0001, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=True, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type='p2p', enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, symmetric_ar_type=None, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, mamba_num_heads=None, use_mamba_mem_eff_path=True, mlp_chunks_for_prefill=1, heterogeneous_block_specs=False, hetereogenous_dist_checkpoint=False, quant_recipe=None, q_lora_rank=None, kv_lora_rank=512, qk_head_dim=128, qk_pos_emb_head_dim=64, v_head_dim=128, rope_type='yarn', rotary_base=10000, rotary_percent=1.0, rotary_scaling_factor=1.0, max_position_embeddings=4096, original_max_position_embeddings=4096, beta_fast=32, beta_slow=1, mscale=1.0, mscale_all_dim=1.0)MLATransformerConfig(tensor_model_parallel_size=1, pipeline_model_parallel_comm_backend=None, pipeline_model_parallel_size=1, virtual_pipeline_model_parallel_size=None, sequence_parallel=False, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=4, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=None, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=True, async_tensor_model_parallel_allreduce=True, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=False, overlap_p2p_comm=False, batch_p2p_comm=True, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=True, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=1, delay_wgrad_compute=False, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=28, mtp_num_layers=None, mtp_loss_scaling_factor=0.1, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, pipeline_model_parallel_layout=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=2048, num_attention_heads=16, attention_backend=<AttnBackend.auto: 5>, softmax_scale=None, num_query_groups=16, ffn_hidden_size=10944, kv_channels=128, hidden_dropout=0.0, attention_dropout=0.0, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-06, layernorm_zero_centered_gamma=False, add_bias_linear=False, add_qkv_bias=False, gated_linear_unit=True, activation_func=<function silu at 0x7f325ee6cc10>, activation_func_fp8_input_store=False, num_moe_experts=64, rotary_interleaved=False, window_size=None, normalization='RMSNorm', qk_layernorm=True, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=True, no_rope_freq=None, moe_deepep_num_sms=20, init_method=functools.partial(<function normal_ at 0x7f325ed05480>, mean=0.0, std=0.006), output_layer_init_method=functools.partial(<function normal_ at 0x7f325ed05480>, mean=0.0, std=0.0008017837257372732), init_method_std=0.006, embedding_init_method=functools.partial(<function normal_ at 0x7f325ed05480>, mean=0.0, std=0.006), embedding_init_method_std=0.006, init_model_with_meta_device=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, disable_bf16_reduced_precision_matmul=False, bias_activation_fusion=True, masked_softmax_fusion=False, persist_layer_norm=True, memory_efficient_layer_norm=False, bias_dropout_fusion=False, apply_rope_fusion=False, recompute_granularity='full', recompute_method='uniform', recompute_num_layers=1, distribute_saved_activations=False, recompute_modules=['core_attn'], fp8=None, fp8_recipe='delayed', fp8_param=False, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=True, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, use_kitchen=False, moe_shared_expert_intermediate_size=2816, moe_shared_expert_overlap=False, moe_layer_freq=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], moe_ffn_hidden_size=1408, moe_router_load_balancing_type='aux_loss', moe_router_topk=6, moe_router_topk_limited_devices=None, moe_router_padding_for_fp8=False, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_pre_softmax=True, moe_router_topk_scaling_factor=2.446, moe_router_score_function='softmax', moe_router_dtype='fp32', moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_router_force_load_balancing=False, moe_grouped_gemm=True, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0.001, moe_z_loss_coeff=0.0001, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_per_layer_logging=True, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, moe_permute_fusion=False, moe_apply_probs_on_input=False, cp_comm_type='p2p', enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False, symmetric_ar_type=None, mrope_section=None, is_hybrid_model=False, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, mamba_num_heads=None, use_mamba_mem_eff_path=True, mlp_chunks_for_prefill=1, heterogeneous_block_specs=False, hetereogenous_dist_checkpoint=False, quant_recipe=None, q_lora_rank=None, kv_lora_rank=512, qk_head_dim=128, qk_pos_emb_head_dim=64, v_head_dim=128, rope_type='yarn', rotary_base=10000, rotary_percent=1.0, rotary_scaling_factor=1.0, max_position_embeddings=4096, original_max_position_embeddings=4096, beta_fast=32, beta_slow=1, mscale=1.0, mscale_all_dim=1.0)

/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
/usr/lib/python3.10/contextlib.py:103: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[rank221]: Traceback (most recent call last):
[rank221]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/examples/deepseek-v2/pretrain_deepseekv2.py", line 356, in <module>
[rank221]:     pretrain(
[rank221]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/training/training.py", line 898, in pretrain
[rank221]:     iteration, num_floating_point_operations_so_far = train(
[rank221]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/training.py", line 767, in train
[rank221]:     update_num_microbatches(args.consumed_train_samples, consistency_check=True, verbose=True)
[rank221]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/core/num_microbatches_calculator.py", line 51, in update_num_microbatches
[rank221]:     _GLOBAL_NUM_MICROBATCHES_CALCULATOR.update(consumed_samples, consistency_check, verbose)
[rank221]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/core/num_microbatches_calculator.py", line 476, in update
[rank221]:     self.current_global_batch_size % self.micro_batch_times_data_parallel_size == 0
[rank221]: AssertionError: current global batch size (3600) is not divisible by micro-batch-size (5) timesdata parallel size (480)
[rank220]: Traceback (most recent call last):
[rank220]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/examples/deepseek-v2/pretrain_deepseekv2.py", line 356, in <module>
[rank220]:     pretrain(
[rank220]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/training/training.py", line 898, in pretrain
[rank220]:     iteration, num_floating_point_operations_so_far = train(
[rank220]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/training.py", line 767, in train
[rank220]:     update_num_microbatches(args.consumed_train_samples, consistency_check=True, verbose=True)
[rank220]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/core/num_microbatches_calculator.py", line 51, in update_num_microbatches
[rank220]:     _GLOBAL_NUM_MICROBATCHES_CALCULATOR.update(consumed_samples, consistency_check, verbose)
[rank220]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/core/num_microbatches_calculator.py", line 476, in update
[rank220]:     self.current_global_batch_size % self.micro_batch_times_data_parallel_size == 0
[rank220]: AssertionError: current global batch size (3600) is not divisible by micro-batch-size (5) timesdata parallel size (480)
[rank223]: Traceback (most recent call last):
[rank223]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/examples/deepseek-v2/pretrain_deepseekv2.py", line 356, in <module>
[rank223]:     pretrain(
[rank223]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/training/training.py", line 898, in pretrain
[rank223]:     iteration, num_floating_point_operations_so_far = train(
[rank223]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/training.py", line 767, in train
[rank223]:     update_num_microbatches(args.consumed_train_samples, consistency_check=True, verbose=True)
[rank223]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/core/num_microbatches_calculator.py", line 51, in update_num_microbatches
[rank223]:     _GLOBAL_NUM_MICROBATCHES_CALCULATOR.update(consumed_samples, consistency_check, verbose)
[rank223]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/core/num_microbatches_calculator.py", line 476, in update
[rank223]:     self.current_global_batch_size % self.micro_batch_times_data_parallel_size == 0
[rank223]: AssertionError: current global batch size (3600) is not divisible by micro-batch-size (5) timesdata parallel size (480)
[rank219]: Traceback (most recent call last):
[rank219]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/examples/deepseek-v2/pretrain_deepseekv2.py", line 356, in <module>
[rank219]:     pretrain(
[rank219]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/training/training.py", line 898, in pretrain
[rank219]:     iteration, num_floating_point_operations_so_far = train(
[rank219]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/training.py", line 767, in train
[rank219]:     update_num_microbatches(args.consumed_train_samples, consistency_check=True, verbose=True)
[rank219]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/core/num_microbatches_calculator.py", line 51, in update_num_microbatches
[rank219]:     _GLOBAL_NUM_MICROBATCHES_CALCULATOR.update(consumed_samples, consistency_check, verbose)
[rank219]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/core/num_microbatches_calculator.py", line 476, in update
[rank219]:     self.current_global_batch_size % self.micro_batch_times_data_parallel_size == 0
[rank219]: AssertionError: current global batch size (3600) is not divisible by micro-batch-size (5) timesdata parallel size (480)
[rank216]: Traceback (most recent call last):
[rank216]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/examples/deepseek-v2/pretrain_deepseekv2.py", line 356, in <module>
[rank216]:     pretrain(
[rank216]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/training/training.py", line 898, in pretrain
[rank216]:     iteration, num_floating_point_operations_so_far = train(
[rank216]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/training.py", line 767, in train
[rank216]:     update_num_microbatches(args.consumed_train_samples, consistency_check=True, verbose=True)
[rank216]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/core/num_microbatches_calculator.py", line 51, in update_num_microbatches
[rank216]:     _GLOBAL_NUM_MICROBATCHES_CALCULATOR.update(consumed_samples, consistency_check, verbose)
[rank216]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/core/num_microbatches_calculator.py", line 476, in update
[rank216]:     self.current_global_batch_size % self.micro_batch_times_data_parallel_size == 0
[rank216]: AssertionError: current global batch size (3600) is not divisible by micro-batch-size (5) timesdata parallel size (480)
[rank218]: Traceback (most recent call last):
[rank218]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/examples/deepseek-v2/pretrain_deepseekv2.py", line 356, in <module>
[rank218]:     pretrain(
[rank218]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/training/training.py", line 898, in pretrain
[rank218]:     iteration, num_floating_point_operations_so_far = train(
[rank218]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/training.py", line 767, in train
[rank218]:     update_num_microbatches(args.consumed_train_samples, consistency_check=True, verbose=True)
[rank218]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/core/num_microbatches_calculator.py", line 51, in update_num_microbatches
[rank218]:     _GLOBAL_NUM_MICROBATCHES_CALCULATOR.update(consumed_samples, consistency_check, verbose)
[rank218]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/core/num_microbatches_calculator.py", line 476, in update
[rank218]:     self.current_global_batch_size % self.micro_batch_times_data_parallel_size == 0
[rank218]: AssertionError: current global batch size (3600) is not divisible by micro-batch-size (5) timesdata parallel size (480)
[rank222]: Traceback (most recent call last):
[rank222]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/examples/deepseek-v2/pretrain_deepseekv2.py", line 356, in <module>
[rank222]:     pretrain(
[rank222]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/training/training.py", line 898, in pretrain
[rank222]:     iteration, num_floating_point_operations_so_far = train(
[rank222]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/training.py", line 767, in train
[rank222]:     update_num_microbatches(args.consumed_train_samples, consistency_check=True, verbose=True)
[rank222]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/core/num_microbatches_calculator.py", line 51, in update_num_microbatches
[rank222]:     _GLOBAL_NUM_MICROBATCHES_CALCULATOR.update(consumed_samples, consistency_check, verbose)
[rank222]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/core/num_microbatches_calculator.py", line 476, in update
[rank222]:     self.current_global_batch_size % self.micro_batch_times_data_parallel_size == 0
[rank222]: AssertionError: current global batch size (3600) is not divisible by micro-batch-size (5) timesdata parallel size (480)
[rank217]: Traceback (most recent call last):
[rank217]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/examples/deepseek-v2/pretrain_deepseekv2.py", line 356, in <module>
[rank217]:     pretrain(
[rank217]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/training/training.py", line 898, in pretrain
[rank217]:     iteration, num_floating_point_operations_so_far = train(
[rank217]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/megatron-lm-musa-patch/musa_patch/training.py", line 767, in train
[rank217]:     update_num_microbatches(args.consumed_train_samples, consistency_check=True, verbose=True)
[rank217]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/core/num_microbatches_calculator.py", line 51, in update_num_microbatches
[rank217]:     _GLOBAL_NUM_MICROBATCHES_CALCULATOR.update(consumed_samples, consistency_check, verbose)
[rank217]:   File "/mnt/moer-train/public/train_32B/Kuae2.1-1022/Megatron-LM/megatron/core/num_microbatches_calculator.py", line 476, in update
[rank217]:     self.current_global_batch_size % self.micro_batch_times_data_parallel_size == 0
[rank217]: AssertionError: current global batch size (3600) is not divisible by micro-batch-size (5) timesdata parallel size (480)
