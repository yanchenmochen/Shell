GPTModel(
  (embedding): LanguageModelEmbedding(
    (word_embeddings): VocabParallelEmbedding()
    (embedding_dropout): Dropout(p=0.0, inplace=False)
  )
  (decoder): TransformerBlock(
    (layers): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): RMSNorm()
        (self_attention): MLASelfAttention(
          (core_attention): TEDotProductAttention(
            (flash_attention): FlashAttention()
            (fused_attention): FusedAttention()
            (unfused_attention): UnfusedDotProductAttention(
              (scale_mask_softmax): FusedScaleMaskSoftmax()
              (attention_dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (linear_proj): TERowParallelLinear(in_features=4096, out_features=2048, bias=False, TP=1)
          (rotary_pos_emb): YarnRotaryEmbedding()
          (linear_q_down_proj): TEColumnParallelLinear(in_features=2048, out_features=1536, bias=False, TP=1)
          (linear_q_up_proj): TELayerNormColumnParallelLinear(in_features=1536, out_features=6144, bias=False, TP=1)
          (linear_kv_down_proj): TEColumnParallelLinear(in_features=2048, out_features=576, bias=False, TP=1)
          (linear_kv_up_proj): TELayerNormColumnParallelLinear(in_features=512, out_features=8192, bias=False, TP=1)
          (q_layernorm): IdentityOp()
          (kv_layernorm): IdentityOp()
        )
        (pre_cross_attn_layernorm): IdentityOp()
        (cross_attention): IdentityOp()
        (cross_attn_bda): IdentityFuncOp()
        (pre_mlp_layernorm): IdentityOp()
        (mlp): MLP(
          (linear_fc1): TELayerNormColumnParallelLinear(in_features=2048, out_features=24576, bias=False, TP=1)
          (linear_fc2): TERowParallelLinear(in_features=12288, out_features=2048, bias=False, TP=1)
        )
      )
      (1-39): 39 x TransformerLayer(
        (input_layernorm): RMSNorm()
        (self_attention): MLASelfAttention(
          (core_attention): TEDotProductAttention(
            (flash_attention): FlashAttention()
            (fused_attention): FusedAttention()
            (unfused_attention): UnfusedDotProductAttention(
              (scale_mask_softmax): FusedScaleMaskSoftmax()
              (attention_dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (linear_proj): TERowParallelLinear(in_features=4096, out_features=2048, bias=False, TP=1)
          (rotary_pos_emb): YarnRotaryEmbedding()
          (linear_q_down_proj): TEColumnParallelLinear(in_features=2048, out_features=1536, bias=False, TP=1)
          (linear_q_up_proj): TELayerNormColumnParallelLinear(in_features=1536, out_features=6144, bias=False, TP=1)
          (linear_kv_down_proj): TEColumnParallelLinear(in_features=2048, out_features=576, bias=False, TP=1)
          (linear_kv_up_proj): TELayerNormColumnParallelLinear(in_features=512, out_features=8192, bias=False, TP=1)
          (q_layernorm): IdentityOp()
          (kv_layernorm): IdentityOp()
        )
        (pre_cross_attn_layernorm): IdentityOp()
        (cross_attention): IdentityOp()
        (cross_attn_bda): IdentityFuncOp()
        (pre_mlp_layernorm): RMSNorm()
        (mlp): MoELayer(
          (router): TopKRouter()
          (experts): TEGroupedMLP(
            (linear_fc1): TEColumnParallelGroupedLinear()
            (linear_fc2): TERowParallelGroupedLinear()
          )
          (shared_experts): SharedExpertMLP(
            (linear_fc1): TEColumnParallelLinear(in_features=2048, out_features=3072, bias=False, TP=1)
            (linear_fc2): TERowParallelLinear(in_features=1536, out_features=2048, bias=False, TP=1)
          )
        )
      )
    )
    (final_layernorm): RMSNorm()
  )
  (output_layer): ColumnParallelLinear(in_features=2048, out_features=128256, bias=False, TP=1)
)
